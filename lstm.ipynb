{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "mount_file_id": "1U87ttTpX0hhDuHtyLlwGaPWSOeq-2vS8",
      "authorship_tag": "ABX9TyNe3mEQOp2sefAf/s20sr1M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saeedt70/persian-text-classification-/blob/main/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf49e9jDB6l6"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl\n",
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from hazm import *\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "QR6EnAWpCSXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/Saeed taheri.xlsx')\n",
        "df.shape"
      ],
      "metadata": {
        "id": "NUZkrtkaCWIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud-fa -qqq\n",
        "!pip install gradio -qqq\n",
        "import pandas as pd # to manipulate the dataset\n",
        "from wordcloud_fa import WordCloudFa # to create wordcloud out of persian text\n",
        "import gradio as gr # to build & share delightful apps\n",
        "\n",
        "from collections import Counter\n",
        "text1 = values = ''.join(str(v) for v in df['text'])\n",
        "\n",
        "wc = WordCloudFa( background_color=\"white\", width=1000, height=500)\n",
        "word_cloud = wc.generate(text1)\n",
        "image = word_cloud.to_image()\n",
        "image"
      ],
      "metadata": {
        "id": "leNU0c-BEz6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data set Plot"
      ],
      "metadata": {
        "id": "tYUMPbNRAcgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "isni=df.isnull().sum()\n",
        "print(isni)\n",
        "sns.countplot(x=df[\"cat\"])\n",
        "plt.show()\n",
        "\n",
        "sns.countplot(x=df[\"sent\"])\n",
        "plt.show()\n",
        "#isnull\n",
        "\n"
      ],
      "metadata": {
        "id": "7eptHq_wAV3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "stop=stopwords_list()\n",
        "normalizer = Normalizer()\n",
        "df = df.dropna()\n",
        "df['text'] = df['text'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE).str.lower()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6w2BnpV4CpHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].apply(lambda x:normalizer.normalize(x))\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: re.sub('[0-9]+', ' ', x))\n",
        "df['text'] = df['text'].apply(lambda x: [item for item in x.split() if item not in stop])\n",
        "To_Process = df[['cat','text']]\n",
        "To_Process.shape"
      ],
      "metadata": {
        "id": "S41gkcFwiB7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(To_Process, test_size=0.1, random_state=42, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "W4PWjl29DO_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# Define the sequence lengths, max number of words and embedding dimensions\n",
        "MAX_SEQUENCE_LENGTH = 300 #can plot a graph for length\n",
        "MAX_NB_WORDS = 15000\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# Get the frequently occurring words\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(train.text)\n",
        "train_sequences = tokenizer.texts_to_sequences(train.text)\n",
        "test_sequences = tokenizer.texts_to_sequences(test.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "#some padding shit\n",
        "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "metadata": {
        "id": "kaz_tNECDYmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "train_labels = train['cat']\n",
        "test_labels = test['cat']\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(train_labels)\n",
        "train_labels = le.transform(train_labels)\n",
        "test_labels = le.transform(test_labels)\n",
        "print(le.classes_)"
      ],
      "metadata": {
        "id": "yOLIibCdDsYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#change data types (actualy idk why)\n",
        "labels_train = to_categorical(np.asarray(train_labels))\n",
        "labels_test = to_categorical(np.asarray(test_labels))"
      ],
      "metadata": {
        "id": "8BUkH4qlD2t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.layers import Dropout, Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Activation, Dense,GRU,Flatten,LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
        "model2.add(Bidirectional(LSTM(64,  activation='relu',  dropout=0.2,recurrent_regularizer='l1',return_sequences=True)))\n",
        "#model2.add(GRU(128, recurrent_regularizer='l1',return_sequences=True))\n",
        "#model2.add(Bidirectional(GRU(128)))\n",
        "#model2.add(LSTM(64,  activation='relu',recurrent_regularizer='l1',return_sequences=True))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(9,activation='softmax'))\n",
        "model2.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics = ['accuracy'])\n",
        "print(model2.summary())"
      ],
      "metadata": {
        "id": "5qeX9gRCErlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "zqlmaxbOnuP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histories = {'accuracy':[], 'loss':[], 'val_accuracy':[], 'val_loss':[]}\n",
        "for train_idx, val_idx in kfold.split(train_data):\n",
        "    x_train_f = train_data[train_idx]\n",
        "    y_train_f = labels_train[train_idx]\n",
        "    x_val_f = train_data[val_idx]\n",
        "    y_val_f = labels_train[val_idx] \n",
        "\n",
        "    history=model2.fit(x_train_f, y_train_f, batch_size=64,validation_data=(x_val_f, y_val_f), epochs=10,verbose = 1)\n",
        "    histories['accuracy'].append(history.history['accuracy'])\n",
        "    histories['loss'].append(history.history['loss'])\n",
        "    histories['val_accuracy'].append(history.history['val_accuracy'])\n",
        "    histories['val_loss'].append(history.history['val_loss'])"
      ],
      "metadata": {
        "id": "FQFR930L9SgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model2.save('bigru0')"
      ],
      "metadata": {
        "id": "1ZwyfckyT3mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6dEmBoZwYTMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#plot"
      ],
      "metadata": {
        "id": "YiBTRCMn7c5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accr= model2.evaluate(test_data,labels_test)\n",
        "print(accr)\n",
        "print('accuracy')\n",
        "print(histories['accuracy'])\n",
        "print('loss')\n",
        "print(histories['loss'])\n",
        "print('val_accuracy')\n",
        "print(histories['val_accuracy'])\n",
        "print('val_loss')\n",
        "print(histories['val_loss'])\n",
        "\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot( histories['accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('fold')\n",
        "#plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "###########los\n",
        "plt.plot( histories['loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('fold')\n",
        "#plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "################################\n",
        "\n",
        "plt.plot(histories['val_loss'])\n",
        "plt.title('val_losss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('fold')\n",
        "#plt.legend(['accuracy', 'loss'], loc='upper left')\n",
        "plt.show()\n",
        "################################\n",
        "plt.plot(histories['val_accuracy'])\n",
        "plt.title(' val-accuracy ')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('fold')\n",
        "#plt.legend(['val-accuracy', 'val-loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eMOIx8gA53BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "predicted=model2.predict(test_data)\n",
        "predicted\n",
        "\n",
        "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))\n",
        "print('fscore: {}'.format(fscore))\n",
        "print('support: {}'.format(support))\n",
        "print(\"############################\")\n",
        "print(sklearn.metrics.classification_report(labels_test, predicted.round()))\n",
        "\n",
        "###############################\n"
      ],
      "metadata": {
        "id": "QhZar-BNS1zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "target =  ['ghimat', 'internet', 'fun', 'hard','bag','time&level','update','other','spam']\n",
        "#target =  ['manfi', 'khonsa', 'mosbat'] \n",
        "report1 =sklearn.metrics.classification_report(labels_test, predicted.round(),target_names=target, digits=4,  output_dict=True)\n",
        "display.display(pd.DataFrame(report1))\n",
        "df = pd.DataFrame(report1)\n",
        "df.iloc[:3, :9].T.plot(kind='bar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VMU4jQ7Xd54U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted"
      ],
      "metadata": {
        "id": "Q_08iJkyVU4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 1024\n",
        "print(test.iloc[k])\n",
        "x = model2(test_data[k:k+1,:])\n",
        "x\n"
      ],
      "metadata": {
        "id": "nwtCQ7zQTOHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import *\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "y_pred = np.argmax(predicted.round(), axis=1)\n",
        "cm=multilabel_confusion_matrix(test_labels, y_pred, labels=[1, 2, 3,4,5,6,7,8,9] )\n",
        "print(cm)\n",
        "\n",
        "#############\n",
        "# Plot confusion matrix \n",
        "fig = plt.figure(figsize = (15, 10))\n",
        "#['manfi', 'khonsa', 'mosbat']\n",
        "#['ghimat', 'internet', 'fun', 'hard','bag','time&level','update','other','spam']\n",
        "for i, (label, matrix) in enumerate(zip(['ghimat', 'internet', 'fun', 'hard','bag','time&level','update','other','spam'], cm)):\n",
        "    plt.subplot(f'33{i+1}')\n",
        "    labels = [f'{label}', label]\n",
        "    sns.heatmap(matrix, annot = True, square = True, fmt = 'd', cbar = False, cmap = 'Blues', \n",
        "                xticklabels = labels, yticklabels = labels, linecolor = 'black', linewidth = 1)\n",
        "    plt.title(labels[0])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OcuSXU8Gin91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}